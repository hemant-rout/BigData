import java.util.regex.Pattern
def decimalStrip(inputValue: String, decimal_point: String): String = {
    if (inputValue == null) return null
    val filter = inputValue.replaceAll("[^0-9-\\" + decimal_point + "]", "")

    if (!(filter == "")) {
      val regx = "^(-)(0+)(.*)|^(0+)(.*)"
      val `match` = Pattern.compile(regx).matcher(filter)
      if (`match`.find) if (`match`.group(1) != null) return `match`.group(1) + `match`.group(3)
      else return `match`.group(5)
      return filter
    }
    "0"
  }


 def stringFilter(inputValue: String, searchString: String): String = {
    if (inputValue == null) return null
    val inputChars = inputValue.toCharArray
    val searchStringChars = searchString.toCharArray
    val returnset = new util.LinkedList[Character]
    var i = 0
    while (i < inputChars.length) {
      {
        var j = 0
        while (j < searchStringChars.length) {
          {
            if (inputChars(i) == searchStringChars(j)) {
              returnset.add(inputChars(i))
              break //todo: break is not supported
            }
          }
          {
            j += 1; j - 1
          }
        }
      }
      {
        i += 1; i - 1
      }
    }
    var returnString = ""
    import scala.collection.JavaConversions._
    for (c <- returnset) {
      returnString += c
    }
    returnString
  }


def stringLeftTrim(inputValue: String): String = {
    if (inputValue == null) return null
    inputValue.replaceFirst("\\s+", "")
  }

https://alvinalexander.com/scala/how-find-difference-intersection-distinct-characters-in-string

  val rdd:RDD[CassandraRow]=spark.sparkContext.parallelize(Seq(
      CassandraRow.fromMap(Map("puid"->"p1","vuid"->"v1","e1"->1,"e2"->null,"e3"->1,"edate"->"2015-01-02 00:00:00.000")),
      CassandraRow.fromMap(Map("puid"->"p1","vuid"->"v2","e1"->null,"e2"->1,"e3"->1,"edate"->"2016-01-02 00:00:00.000")),
      CassandraRow.fromMap(Map("puid"->"p1","vuid"->"v3","e1"->null,"e2"->null,"e3"->1,"edate"->"2017-01-02 00:00:00.000")),
      CassandraRow.fromMap(Map("puid"->"p2","vuid"->"v1","e1"->1,"e2"->null,"e3"->null,"edate"->"2017-08-02 00:00:00.000")),
      CassandraRow.fromMap(Map("puid"->"p2","vuid"->"v2","e1"->1,"e2"->1,"e3"->null,"edate"->"2017-01-06 00:00:00.000")),
      CassandraRow.fromMap(Map("puid"->"p3","vuid"->"v1","e1"->1,"e2"->null,"e3"->null,"edate"->"2017-01-06 00:00:00.000"))
    ))

  countElement(rdd,"e1","e2","e3")
  

  def countElement(rdd1:RDD[CassandraRow],elements:String*)
  {
    val countList=rdd1.filter({
      r=> elements.filter(element=> !r.isNullAt(element)).size>1
    }).map(r=>(r.getString("puid"),r.getDate("edate"))).collect()
  rdd1.filter(r=> countList.exists(x=>x._1.equalsIgnoreCase(r.getString("puid")) && (r.getDate("edate").after(x._2) ||r.getDate("edate").equals(x._2)))).foreach(println)
  }




 object Main extends App {

    object WeekDay extends Enumeration {
      type WeekDay = Value
      val Mon, Tue, Wed, Thu, Fri, Sat, Sun = Value
    }
    import WeekDay._

    def isWorkingDay(d: WeekDay) = ! (d == Sat || d == Sun)

    WeekDay.values filter isWorkingDay foreach println
  }


    object WeekDay extends Enumeration {
      type WeekDay = Value
      val Sat=Value("Sat")
      val Fri=Value("ge")
      val Sun=Value("Sun")
      //val Mon, Tue, Wed, Thu, Fri, Sat, Sun = Val
    }
    import WeekDay._
    isWorkingDay(WeekDay.Sun)
    def isWorkingDay(d: WeekDay.Value): Unit ={
      println(d)
    }



    /*val df = spark.createDataFrame(Seq(
      (0, "a1", "b1", "c1", "d1"),
      (1, "a2", "b2", "c2", "d2"),
      (2, "a3", "b3", null, "d3"),
      (3, null, null, "c4", "d4"),
      (4, null, "b5", "c5", "d5"),
      (5, null, "b5", "c5", "d5")
    )).toDF("id", "col1", "col2", "col3", "col4")*/

    val df = spark.createDataFrame(Seq(
      (0, "a1", "b1", "12f-puhrns-3435", "d1"),
      (1, "a2", "b2", "12f-puhrns-343556", "d2"),
      (2, "a3", "b3", null, "d3"),
      (3, null, null, "sdfsd-234-6787", "d4"),
      (4, null, "b5", "12f-puhrns-343522", "d5"),
      (5, null, "b5", "12f-puhrns-343522", "d5")
    )).toDF("id", "col1", "col2", "col3", "col4")

    val fieldValue=Array("12f-puhrns-343522","12f-puhrns-343556")

    df.where(df.col("col3").isin(fieldValue:_*)).show()


 val fieldValue=Array("12f-puhrns-343522","12f-puhrns-343556")

    //df.where("col3 =='12f-puhrns-343522' or col3 =='12f-puhrns-343556'").show()

    //df.where("col3 in ('12f-puhrns-343522','12f-puhrns-343556')").show()
    val cond="col3 in ("+fieldValue.map(x=>s"'$x'").mkString(",")+")"
    println(cond)
    df.where(cond).show()
df.where(cond +" or "+"col4 in ('d3')").show()



data = [(1,340.0,10),(1,None,34),(3,200.0,67),(4,np.NAN,86)]
df = spark.createDataFrame(data).toDF(*["a","b","c"]).withColumn("ar",array("a","b","c")).withColumn("ar2",expr("filter(ar, x -> ((x IS NOT NULL) AND (!isNan(x))))"))


---------------------------------------

parsed_code = ast.parse(code)

# Function to extract Spark SQL calls
def extract_spark_sql_calls(node):
    if isinstance(node, ast.Call):
        if isinstance(node.func, ast.Attribute):
            # Check if it's a method call on an object (e.g., spark.sql)
            obj = node.func.value
            method = node.func.attr
            if isinstance(obj, ast.Name) and obj.id == "spark" and method == "sql":
                # Spark SQL method call detected
                sql_query = ast.dump(node.args[0])
                print(f"Spark SQL Call: {sql_query}")

# Traverse the AST and extract Spark SQL calls
for node in ast.walk(parsed_code):
    extract_spark_sql_calls(node)
In this code, we traverse the AST and check if a function call is made on the spark object using the sql method. We then extract the SQL query from the function call using ast.dump(node.args[0]).

When you run this code with the provided sample code, it will print the detected Spark SQL calls:

sql
Copy code
Spark SQL Call: "SELECT * FROM my_table WHERE age > 30"
Keep in mind that this is a basic example and may not cover all possible ways Spark SQL can be used in code. You may need to adapt and extend the code to cover more complex usage scenarios.





parsed_code = ast.parse(code)

# Function to extract Spark SQL calls
def extract_spark_sql_calls(node):
    if isinstance(node, ast.Call):
        if isinstance(node.func, ast.Attribute):
            # Check if it's a method call on an object (e.g., spark.sql)
            obj = node.func.value
            method = node.func.attr
            if isinstance(obj, ast.Name) and obj.id == "spark" and method == "sql":
                # Spark SQL method call detected
                sql_query = ast.dump(node.args[0])
                print(f"Spark SQL Call: {sql_query}")

# Traverse the AST and extract Spark SQL calls
for node in ast.walk(parsed_code):
    extract_spark_sql_calls(node)
In this code, we traverse the AST and check if a function call is made on the spark object using the sql method. We then extract the SQL query from the function call using ast.dump(node.args[0]).

When you run this code with the provided sample code, it will print the detected Spark SQL calls:

sql
Copy code
Spark SQL Call: "SELECT * FROM my_table WHERE age > 30"
Keep in mind that this is a basic example and may not cover all possible ways Spark SQL can be used in code. You may need to adapt and extend the code to cover more complex usage scenarios.


from collections import defaultdict, deque

def compute_ranks(child_parent):
    # Step 1: Build parent-to-children mapping
    parent_children = defaultdict(list)
    all_nodes = set(child_parent.keys()) | set(child_parent.values())  # All unique nodes

    for child, parent in child_parent.items():
        parent_children[parent].append(child)

    # Step 2: Find root nodes (nodes that are never children)
    children = set(child_parent.keys())
    roots = [node for node in all_nodes if node not in children]  # Nodes that are not children

    # Step 3: BFS to compute ranks (incrementing with depth)
    ranks = {node: 99 for node in all_nodes}  # Default all to 99 (assuming leaf nodes)
    queue = deque([(root, 1) for root in roots])  # Start BFS with root nodes at rank 1

    while queue:
        node, rank = queue.popleft()
        if node in parent_children:  # If node has children, assign rank and process children
            ranks[node] = rank
            for child in parent_children[node]:
                queue.append((child, rank + 1))

    return ranks

# Given data (child â†’ parent mapping)
child_parent = {"B": "A", "C": "A", "D": "B", "E": "D"}

# Compute ranks
ranks = compute_ranks(child_parent)

# Print results
for node, rank in sorted(ranks.items()):
    print(f"Node {node}: Rank {rank}")




